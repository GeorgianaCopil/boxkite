{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boxkite demo\n",
    "\n",
    "This demo shows an end-to-end model training in Kubeflow notebooks, model management in MLflow, and monitoring of model input & output distributions (data and concept drift) in Prometheus + Grafana.\n",
    "\n",
    "## Train a diabetes model\n",
    "\n",
    "First we import our dependencies and turn on mlflow auto-logging, so that the sklearn model we train will automatically have its parameters and metrics logged to MLflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from boxkite.monitoring.service import ModelMonitoringService\n",
    "\n",
    "import mlflow\n",
    "mlflow.sklearn.autolog()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we train a diabetes model on our input data. We save the model to MLflow, and at the same time, we record the histogram (statistical distribution) of the features (input data) and inferences (predictions) using boxkite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run() as run:\n",
    "\n",
    "    bunch = load_diabetes()\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "        bunch.data, bunch.target\n",
    "    )\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, Y_train)\n",
    "\n",
    "    print(\"Score: %.2f\" % model.score(X_test, Y_test))\n",
    "    with open(\"./model.pkl\", \"wb\") as f:\n",
    "        pickle.dump(model, f)\n",
    "\n",
    "    # features = [(\"age\", [33, 23, 54, ...]), (\"sex\", [0, 1, 0]), ...]\n",
    "    features = zip(*[bunch.feature_names, X_train.T])\n",
    "    \n",
    "    Y_pred = model.predict(X_test)\n",
    "    inference = list(Y_pred)\n",
    "    \n",
    "    ModelMonitoringService.export_text(\n",
    "        features=features, inference=inference, path=\"./histogram.txt\",\n",
    "    )\n",
    "    mlflow.log_artifact(\"./histogram.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model has been automatically logged to MLflow against the `run.info.run_id`. This is a unique identifier from MLflow which pins down the exact model version along with the histogram we saved from the training data and the predictions the model gave on it.\n",
    "\n",
    "So let's check we can get it out of MLflow and run some inferences on it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logged_model = f\"s3://mlflow-artifacts/0/{run.info.run_id}/artifacts/model\"\n",
    "\n",
    "# Load model as a PyFuncModel.\n",
    "loaded_model = mlflow.pyfunc.load_model(logged_model)\n",
    "\n",
    "# Predict on a Pandas DataFrame.\n",
    "import pandas as pd\n",
    "loaded_model.predict(pd.DataFrame(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good! Now let's deploy the model to the same Kubernetes cluster we're running on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy the model to production in HA mode\n",
    "\n",
    "Now we're going to deploy the code in https://github.com/boxkite-ml/boxkite/blob/master/examples/kubeflow-mlflow/app/serve_completed.py to the Kubernetes cluster. We've already dockerized it for you, so you don't need to.\n",
    "\n",
    "That code will fetch the model and its histogram from MLflow, and serve it along with the `/metrics` endpoint so you can - live - and across multiple model servers - compare both the training time data and inference distributions against the inference time data and inference distributions.\n",
    "\n",
    "The key parts of the code (see above link for the full listing) are:\n",
    "\n",
    "Inferences for the model, note how we log to `monitor.log_prediction`:\n",
    "\n",
    "```python\n",
    "@app.route(\"/\", methods=[\"POST\"])\n",
    "def predict():\n",
    "    features = request.json\n",
    "    score = model.predict([features])[0]\n",
    "    pid = monitor.log_prediction(\n",
    "        request_body=request.data,\n",
    "        features=features,\n",
    "        output=score,\n",
    "    )\n",
    "    return {\"result\": score, \"prediction_id\": pid}\n",
    "\n",
    "```\n",
    "\n",
    "And where the Prometheus-format metrics are exposed:\n",
    "\n",
    "```python\n",
    "@app.route(\"/metrics\", methods=[\"GET\"])\n",
    "def metrics():\n",
    "    return monitor.export_http()[0]\n",
    "\n",
    "```\n",
    "\n",
    "With these simple changes to your model server, you can now track live data and model drift!\n",
    "\n",
    "Now we'll deploy it to Kubernetes, with some boilerplate deployment and service configuration. The deployment simply says to run three instances of the model server in HA mode, and the service tells Kubernetes to make those services available behind an internal load balancer called `ml-server`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment = f\"\"\"\n",
    "apiVersion: apps/v1\n",
    "kind: Deployment\n",
    "metadata:\n",
    "  name: ml-deployment\n",
    "  labels:\n",
    "    app: ml-server\n",
    "spec:\n",
    "  replicas: 3\n",
    "  selector:\n",
    "    matchLabels:\n",
    "      app: ml-server\n",
    "  template:\n",
    "    metadata:\n",
    "      labels:\n",
    "        app: ml-server\n",
    "      annotations:\n",
    "        prometheus.io/scrape: \"true\"\n",
    "    spec:\n",
    "      containers:\n",
    "      - name: ml-server\n",
    "        image: quay.io/boxkite/boxkite-app:e7a70df\n",
    "        ports:\n",
    "        - containerPort: 5000\n",
    "        #command: [\"tail\", \"-f\", \"/dev/null\"]\n",
    "        env:\n",
    "        - name: MLFLOW_RUN_ID\n",
    "          value: {run.info.run_id}\n",
    "        - name: MLFLOW_TRACKING_URI\n",
    "          value: {os.environ['MLFLOW_TRACKING_URI']}          \n",
    "        - name: MLFLOW_S3_ENDPOINT_URL\n",
    "          value: {os.environ['MLFLOW_S3_ENDPOINT_URL']}\n",
    "        - name: AWS_ACCESS_KEY_ID\n",
    "          value: {os.environ['AWS_ACCESS_KEY_ID']}\n",
    "        - name: AWS_SECRET_ACCESS_KEY\n",
    "          value: {os.environ['AWS_SECRET_ACCESS_KEY']}\n",
    "\"\"\"\n",
    "\n",
    "service = \"\"\"\n",
    "apiVersion: v1\n",
    "kind: Service\n",
    "metadata:\n",
    "  name: ml-server\n",
    "spec:\n",
    "  selector:\n",
    "    app: ml-server\n",
    "  ports:\n",
    "    - protocol: TCP\n",
    "      port: 80\n",
    "      targetPort: 5000\n",
    "\"\"\"\n",
    "open(\"deployment.yaml\", \"w\").write(deployment)\n",
    "open(\"service.yaml\", \"w\").write(service)\n",
    "!kubectl apply -f deployment.yaml\n",
    "!kubectl apply -f service.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we wait for the deployment to come up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl rollout status deployment/ml-deployment "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's send a sample request to the model server:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl ml-server -H \"Content-Type: application/json\" \\\n",
    "-d \"[0.03, 0.05, -0.002, -0.01, 0.04, 0.01, 0.08, -0.04, 0.005, -0.1]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hooray, we got a result value! Now let's run a more substantial load test against the model, to get some interesting data in our dashboard:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python boxkite/examples/kubeflow-mlflow/load.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, now continue with the tutorial in the boxkite docs to see how to log into Grafana and observe the results of our load test."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
